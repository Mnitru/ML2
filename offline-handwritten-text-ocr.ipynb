{"cells":[{"cell_type":"markdown","metadata":{},"source":["# *Offline Handwritten Text Recognition*\n","\n","### The purpose of this notebook is to give a brief idea and a basic approach for offline handwritten text recognition by using segmentation and classification. "]},{"cell_type":"markdown","metadata":{},"source":["## What is Offline Handwritten Text Recognition?\n","Offline handwriting recognition involves the automatic conversion of text in an image into letter codes that are usable within computer and text-processing applications. In simple terms, it is the text extraction from your handwritten notebooks/pages. Why called offline? The point being that there is an online text recognition system, which is referred for text that is digitally generated by using tools like stylus, apple pencil, etc."]},{"cell_type":"markdown","metadata":{},"source":["## Approach\n","\n","* **Step1** :  Build a digit(0-9) + A-Z characters classifier using a CNN architecture.\n","* **Step2** :  Apply character segmentation for the handwritten word image.\n","* **Step3** :  Classify each segmented letter and then get the final word in the image."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","from keras.preprocessing.image import ImageDataGenerator\n","import os\n","import random\n","import cv2\n","import imutils\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.preprocessing import LabelBinarizer\n","from keras.models import Sequential\n","from keras.layers import Dense, Activation, Flatten, MaxPooling2D, Dropout, Conv2D\n","from keras.utils import to_categorical\n","\n","# Directory paths\n","train_dir = '/content/drive/My Drive/training_data'\n","val_dir = '/content/drive/My Drive/testing_data'\n","\n","# Function to load and preprocess images\n","def load_images(directory, img_size, non_chars):\n","    data = []\n","    for label in os.listdir(directory):\n","        if label in non_chars:\n","            continue\n","        count = 0\n","        label_dir = os.path.join(directory, label)\n","        for image_file in os.listdir(label_dir):\n","            count += 1\n","            if count > 4000:  # Limit number of images per class for training\n","                break\n","            img = cv2.imread(os.path.join(label_dir, image_file), 0)  # Load in grayscale\n","            if img is None:\n","                print(f\"Failed to load image {os.path.join(label_dir, image_file)}\")\n","                continue\n","            img = cv2.resize(img, (img_size, img_size))\n","            data.append([img, label])\n","    return data\n","\n","# Load training and validation images\n","train_data = load_images(train_dir, 32, [])\n","val_data = load_images(val_dir, 32, [])\n","\n","# Shuffle the data\n","random.shuffle(train_data)\n","random.shuffle(val_data)\n","\n","# Separate features and labels\n","train_X = np.array([image for image, _ in train_data]) / 255.0\n","train_Y = np.array([label for _, label in train_data])\n","val_X = np.array([image for image, _ in val_data]) / 255.0\n","val_Y = np.array([label for _, label in val_data])\n","\n","# One-hot encode labels\n","LB = LabelBinarizer()\n","train_Y = LB.fit_transform(train_Y)\n","val_Y = LB.transform(val_Y)\n","\n","# Reshape data for CNN input\n","train_X = train_X.reshape(-1, 32, 32, 1)\n","val_X = val_X.reshape(-1, 32, 32, 1)\n","\n","# Model architecture\n","model = Sequential()\n","model.add(Conv2D(32, (3, 3), padding=\"same\", activation='relu', input_shape=(32, 32, 1)))\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","model.add(Conv2D(64, (3, 3), activation='relu'))\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","model.add(Conv2D(128, (3, 3), activation='relu'))\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","model.add(Dropout(0.25))\n","model.add(Flatten())\n","model.add(Dense(128, activation='relu'))\n","model.add(Dropout(0.2))\n","model.add(Dense(36, activation='softmax'))\n","\n","# Compile the model\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","# Train the model\n","history = model.fit(train_X, train_Y, epochs=50, batch_size=32, validation_data=(val_X, val_Y), verbose=1)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["len(train_data)\n","len(val_data)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["plt.plot(history.history['accuracy'])\n","plt.plot(history.history['val_accuracy'])\n","plt.title('Training Accuracy vs Validation Accuracy')\n","plt.ylabel('Accuracy')\n","plt.xlabel('Epoch')\n","plt.legend(['Train', 'Validation'], loc='upper left')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])\n","plt.title('Training Loss vs Validation Loss')\n","plt.ylabel('Loss')\n","plt.xlabel('Epoch')\n","plt.legend(['Train', 'Validation'], loc='upper left')\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["## Recognition and Post-Processing \n","1. The sort contours function is used to get the correct order of individual characters for correct output extraction. In this case for extracting a single word, a left to right sorting of individual characters is needed.\n","2. The get letters function fetches the list of letters and get word function gets the individual word. "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def sort_contours(cnts, method=\"left-to-right\"):\n","    reverse = False\n","    i = 0\n","    if method == \"right-to-left\" or method == \"bottom-to-top\":\n","        reverse = True\n","    if method == \"top-to-bottom\" or method == \"bottom-to-top\":\n","        i = 1\n","    boundingBoxes = [cv2.boundingRect(c) for c in cnts]\n","    (cnts, boundingBoxes) = zip(*sorted(zip(cnts, boundingBoxes),\n","    key=lambda b:b[1][i], reverse=reverse))\n","    # return the list of sorted contours and bounding boxes\n","    return (cnts, boundingBoxes)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def get_letters(img):\n","    image = cv2.imread(img)\n","    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n","    blur = cv2.GaussianBlur(gray, (5, 5), 0)\n","    edged = cv2.Canny(blur, 30, 150)\n","\n","    contours, _ = cv2.findContours(edged.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n","    letter_image_regions = []\n","    for contour in contours:\n","        (x, y, w, h) = cv2.boundingRect(contour)\n","        if w / h > 1.25:\n","            half_width = int(w / 2)\n","            letter_image_regions.append((x, y, half_width, h))\n","            letter_image_regions.append((x + half_width, y, half_width, h))\n","        else:\n","            letter_image_regions.append((x, y, w, h))\n","\n","    letter_image_regions = sorted(letter_image_regions, key=lambda x: x[0])\n","    letters = []\n","    for box in letter_image_regions:\n","        x, y, w, h = box\n","        cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n","        roi = gray[y:y + h, x:x + w]\n","        try:\n","            roi = cv2.resize(roi, (32, 32), interpolation=cv2.INTER_CUBIC)\n","        except cv2.error as e:\n","            print(f\"Error resizing image: {e}\")\n","            continue\n","        letters.append(roi)\n","    return letters, image"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import pytesseract\n","def get_word(letters):\n","    word = \"\"\n","    for letter in letters:\n","        try:\n","            char = pytesseract.image_to_string(letter)\n","            word += char\n","        except pytesseract.TesseractError as e:\n","            print(f\"Error during OCR: {e}\")\n","    return word"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["letter,image = get_letters(\"../input/handwriting-recognition/train_v2/train/TRAIN_00003.jpg\")\n","word = get_word(letter)\n","print(word)\n","plt.imshow(image)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["letter,image = get_letters(\"../input/handwriting-recognition/train_v2/train/TRAIN_00023.jpg\")\n","word = get_word(letter)\n","print(word)\n","plt.imshow(image)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["letter,image = get_letters(\"../input/handwriting-recognition/train_v2/train/TRAIN_00030.jpg\")\n","word = get_word(letter)\n","print(word)\n","plt.imshow(image)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["letter,image = get_letters(\"../input/handwriting-recognition/validation_v2/validation/VALIDATION_0005.jpg\")\n","word = get_word(letter)\n","print(word)\n","plt.imshow(image)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["letter,image = get_letters(\"../input/handwriting-recognition/test_v2/test/TEST_0007.jpg\")\n","word = get_word(letter)\n","print(word)\n","plt.imshow(image)"]},{"cell_type":"markdown","metadata":{},"source":["## Drawbacks\n","1. The recognition part is dependent on the contour detection code, so if the opencv library is not able to find the character contour, then this method will fail.\n","2. There could be a lot of variation in a single handwritten letter in terms of writing style, therefore a lot more examples are needed for training this model.\n","3. This model will not work for connected texts like a cursive handwritten word."]},{"cell_type":"markdown","metadata":{},"source":["## Conclusion \n","This notebook is an illustration of how a character segmentation and classification approach can be used for offline handwritten text extraction. In order to improve the model, the model should be trained on the complete dataset, this notebook was trained on slightly less number of images due to session constraints. Also, for applying this method to a complete paragraph, following approach can be used, **line segmentation >> word segmentation >> character segmentation >> classification >> post-processing**. "]},{"cell_type":"markdown","metadata":{},"source":["## References\n","1. [https://www.pyimagesearch.com/2020/08/24/ocr-handwriting-recognition-with-opencv-keras-and-tensorflow/](http://) \n","2. [https://www.pyimagesearch.com/2015/04/20/sorting-contours-using-python-and-opencv/](http://)"]},{"cell_type":"markdown","metadata":{},"source":["If you liked this notebook, then do **Upvote** as it will keep me motivated in creating such kernels ahead. **Thanks!!**"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":53376,"sourceId":101598,"sourceType":"datasetVersion"},{"datasetId":818027,"sourceId":1400106,"sourceType":"datasetVersion"}],"dockerImageVersionId":30042,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"}},"nbformat":4,"nbformat_minor":4}
